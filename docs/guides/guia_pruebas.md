# GuÃ­a de Pruebas - Sistema de GeneraciÃ³n de Grafos

## IntroducciÃ³n

Este documento explica cÃ³mo ejecutar, interpretar y extender las pruebas del sistema de generaciÃ³n de grafos. Las pruebas estÃ¡n organizadas en dos categorÃ­as principales: **unitarias** e **integraciÃ³n**.

## Estructura de Pruebas

```
tests/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ run_tests.py              # Script principal de ejecuciÃ³n
â”œâ”€â”€ unit/                     # Pruebas unitarias
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_generador_grafo.py
â”‚   â””â”€â”€ test_utilidades_grafo.py
â””â”€â”€ integration/              # Pruebas de integraciÃ³n
    â”œâ”€â”€ __init__.py
    â””â”€â”€ test_sistema_completo.py
```

## Ejecutar Pruebas

### OpciÃ³n 1: Script Automatizado (Recomendado)

```powershell
# Ejecutar todas las pruebas
python tests\run_tests.py

# Solo pruebas unitarias
python tests\run_tests.py --unit

# Solo pruebas de integraciÃ³n  
python tests\run_tests.py --integration

# Con salida detallada
python tests\run_tests.py --verbose

# Ver ayuda
python tests\run_tests.py --help
```

### OpciÃ³n 2: Unittest Directo

```powershell
# Desde el directorio raÃ­z del proyecto

# Todas las pruebas
python -m unittest discover tests -v

# Solo unitarias
python -m unittest discover tests/unit -v

# Solo integraciÃ³n
python -m unittest discover tests/integration -v

# Prueba especÃ­fica
python -m unittest tests.unit.test_generador_grafo.TestRolNodo -v
```

### OpciÃ³n 3: EjecuciÃ³n Individual

```powershell
# Ejecutar archivo especÃ­fico
python tests\unit\test_generador_grafo.py
python tests\unit\test_utilidades_grafo.py
python tests\integration\test_sistema_completo.py
```

## Pruebas Unitarias

### test_generador_grafo.py

Cubre las siguientes clases y funcionalidades:

#### TestRolNodo
- âœ… VerificaciÃ³n de roles definidos
- âœ… MÃ©todo `todos_los_roles()`
- âœ… ValidaciÃ³n de tipos de datos

```python
# Ejemplo de ejecuciÃ³n
python -m unittest tests.unit.test_generador_grafo.TestRolNodo.test_roles_definidos -v
```

#### TestConfiguracionRoles
- âœ… ConfiguraciÃ³n por defecto
- âœ… Configuraciones personalizadas
- âœ… ValidaciÃ³n de suma de porcentajes
- âœ… CÃ¡lculo de cantidad de nodos
- âœ… GarantÃ­a de mÃ­nimos (almacenamiento y recarga)

```python
# Verificar configuraciones
python -m unittest tests.unit.test_generador_grafo.TestConfiguracionRoles -v
```

#### TestGeneradorGrafoConectado
- âœ… InicializaciÃ³n y configuraciÃ³n
- âœ… Establecimiento de semillas aleatorias
- âœ… GeneraciÃ³n de grafos bÃ¡sicos y personalizados
- âœ… ValidaciÃ³n de parÃ¡metros
- âœ… DistribuciÃ³n de roles
- âœ… Conectividad garantizada
- âœ… EstadÃ­sticas del grafo
- âœ… Propiedades de aristas
- âœ… Reproducibilidad

### test_utilidades_grafo.py

Cubre las siguientes clases:

#### TestConsultorGrafo
- âœ… BÃºsquedas por nombre, ID y atributos
- âœ… NavegaciÃ³n (vecinos, grados, aristas)
- âœ… ValidaciÃ³n de conectividad
- âœ… AnÃ¡lisis de componentes conexas

#### TestCalculadorDistancias
- âœ… Algoritmo Dijkstra
- âœ… ReconstrucciÃ³n de caminos
- âœ… CÃ¡lculo de distancias
- âœ… Manejo de grafos desconectados
- âœ… Aristas sin peso explÃ­cito

#### TestBuscadorNodos
- âœ… BÃºsquedas por rol
- âœ… Nodo mÃ¡s cercano por rol
- âœ… K nodos mÃ¡s cercanos
- âœ… EstadÃ­sticas por roles
- âœ… Casos especiales (roles inexistentes, mismo rol)

## Pruebas de IntegraciÃ³n

### test_sistema_completo.py

#### TestIntegracionGeneradorUtilidades
- âœ… Flujo completo de generaciÃ³n y anÃ¡lisis
- âœ… BÃºsqueda Ã³ptima de servicios
- âœ… AnÃ¡lisis de rutas mÃºltiples
- âœ… Robustez ante fallos simulados
- âœ… Escalabilidad de bÃºsquedas

#### TestCasosUsoReales
- âœ… SimulaciÃ³n de red logÃ­stica
- âœ… SimulaciÃ³n de red IoT
- âœ… SimulaciÃ³n de red CDN
- âœ… OptimizaciÃ³n de configuraciones

#### TestRendimientoYEscalabilidad
- âœ… Rendimiento en generaciÃ³n
- âœ… Escalabilidad de algoritmos
- âœ… Consistencia en mÃºltiples ejecuciones

## InterpretaciÃ³n de Resultados

### Salida Exitosa

```
======================================================================
REPORTE DE RESULTADOS
======================================================================
â±ï¸  Tiempo total: 2.45 segundos
ğŸ§ª Pruebas ejecutadas: 75
âœ… Pruebas exitosas: 75
âŒ Pruebas fallidas: 0
ğŸ’¥ Errores: 0
ğŸš« Omitidas: 0
ğŸ“Š Porcentaje de Ã©xito: 100.0%

======================================================================
ğŸ‰ TODAS LAS PRUEBAS PASARON EXITOSAMENTE
======================================================================
```

### Identificar Problemas

#### Fallo de AserciÃ³n
```
âŒ test_configuracion_invalida_suma_incorrecta
   Expected ValueError but none was raised
```

**SoluciÃ³n**: Verificar que las validaciones estÃ©n implementadas correctamente.

#### Error de ImportaciÃ³n
```
ğŸ’¥ test_crear_grafo_conectado_basico
   ModuleNotFoundError: No module named 'model'
```

**SoluciÃ³n**: Ejecutar desde el directorio raÃ­z del proyecto.

#### Error de Timeout/Rendimiento
```
âŒ test_escalabilidad_busquedas
   AssertionError: 6.2 not less than 5.0
```

**SoluciÃ³n**: Optimizar algoritmos o ajustar lÃ­mites de tiempo.

## Cobertura de Pruebas

### MÃ³dulos Cubiertos

| MÃ³dulo | Cobertura | Pruebas |
|--------|-----------|---------|
| `generador_grafo.py` | 95%+ | 25 pruebas |
| `utilidades_grafo.py` | 90%+ | 35 pruebas |
| IntegraciÃ³n | 100% | 15 pruebas |

### Funcionalidades Principales

- âœ… **GeneraciÃ³n de Grafos**: 100%
- âœ… **Algoritmos de BÃºsqueda**: 95%
- âœ… **CÃ¡lculo de Distancias**: 100%
- âœ… **AnÃ¡lisis de Conectividad**: 100%
- âœ… **EstadÃ­sticas y MÃ©tricas**: 90%
- âœ… **Casos de Uso Reales**: 100%

## Agregar Nuevas Pruebas

### Estructura de Prueba Unitaria

```python
import unittest
from model.tu_modulo import TuClase

class TestTuClase(unittest.TestCase):
    
    def setUp(self):
        """ConfiguraciÃ³n antes de cada prueba"""
        self.objeto = TuClase()
    
    def test_funcionalidad_basica(self):
        """DescripciÃ³n de la prueba"""
        # Arrange (Preparar)
        entrada = "valor_prueba"
        
        # Act (Actuar)
        resultado = self.objeto.tu_metodo(entrada)
        
        # Assert (Verificar)
        self.assertEqual(resultado, valor_esperado)
    
    def test_caso_extremo(self):
        """Prueba de caso extremo"""
        with self.assertRaises(ValueError):
            self.objeto.metodo_que_debe_fallar(entrada_invalida)
    
    def tearDown(self):
        """Limpieza despuÃ©s de cada prueba"""
        pass

if __name__ == '__main__':
    unittest.main(verbosity=2)
```

### Estructura de Prueba de IntegraciÃ³n

```python
class TestIntegracionCompleta(unittest.TestCase):
    
    def setUp(self):
        """ConfiguraciÃ³n para integraciÃ³n"""
        self.sistema = SistemaCompleto()
    
    def test_flujo_completo_caso_uso(self):
        """Prueba de flujo completo"""
        # 1. Preparar datos
        configuracion = ConfiguracionEspecifica()
        
        # 2. Ejecutar flujo
        resultado = self.sistema.ejecutar_flujo_completo(configuracion)
        
        # 3. Verificar resultados
        self.assertIsNotNone(resultado)
        self.assertTrue(resultado.es_valido())
        
        # 4. Verificar efectos secundarios
        self.assertEqual(self.sistema.estado, EstadoEsperado.COMPLETO)
```

## Pruebas de Rendimiento

### MediciÃ³n de Tiempo

```python
import time

def test_rendimiento_algoritmo(self):
    """Prueba de rendimiento"""
    # Configurar datos de prueba
    grafo_grande = self.generar_grafo_grande(1000)
    
    # Medir tiempo
    inicio = time.time()
    resultado = algoritmo_bajo_prueba(grafo_grande)
    tiempo_total = time.time() - inicio
    
    # Verificar rendimiento
    self.assertLess(tiempo_total, 2.0)  # MÃ¡ximo 2 segundos
    self.assertIsNotNone(resultado)
```

### Pruebas de Escalabilidad

```python
def test_escalabilidad_lineal(self):
    """Verifica escalabilidad aproximadamente lineal"""
    tamaÃ±os = [100, 200, 400]
    tiempos = []
    
    for tamaÃ±o in tamaÃ±os:
        grafo = self.generar_grafo(tamaÃ±o)
        inicio = time.time()
        procesar_grafo(grafo)
        tiempos.append(time.time() - inicio)
    
    # Verificar que el crecimiento no sea exponencial
    ratio_crecimiento = tiempos[-1] / tiempos[0]
    ratio_tamaÃ±o = tamaÃ±os[-1] / tamaÃ±os[0]
    
    self.assertLess(ratio_crecimiento, ratio_tamaÃ±o * 2)
```

## Mejores PrÃ¡cticas

### 1. Nombrado de Pruebas

```python
# âœ… Bueno: Descriptivo y claro
def test_crear_grafo_conectado_con_10_nodos_debe_generar_grafo_valido(self):

# âŒ Malo: Muy genÃ©rico
def test_grafo(self):
```

### 2. OrganizaciÃ³n de Pruebas

- **Una clase** por clase bajo prueba
- **Un mÃ©todo** por funcionalidad especÃ­fica
- **Casos separados** para diferentes escenarios

### 3. Datos de Prueba

```python
def setUp(self):
    """Usar datos consistentes"""
    self.generador = GeneradorGrafoConectado()
    self.generador.establecer_semilla(12345)  # Reproducible
    
    self.config_test = ConfiguracionRoles(0.2, 0.3, 0.5)
    self.grafo_pequeÃ±o = self.generador.crear_grafo_conectado(10, 0.3)
```

### 4. Verificaciones Completas

```python
def test_resultado_completo(self):
    resultado = operacion_bajo_prueba()
    
    # Verificar tipo
    self.assertIsInstance(resultado, TipoEsperado)
    
    # Verificar contenido
    self.assertGreater(len(resultado), 0)
    
    # Verificar propiedades especÃ­ficas
    self.assertTrue(resultado.es_valido())
    
    # Verificar invariantes
    self.assertEqual(resultado.suma_total(), valor_esperado)
```

## DepuraciÃ³n de Pruebas

### Usar Debugger

```python
import pdb

def test_con_debug(self):
    resultado = operacion_compleja()
    pdb.set_trace()  # Pausar ejecuciÃ³n aquÃ­
    self.assertEqual(resultado, valor_esperado)
```

### Salida de DepuraciÃ³n

```python
def test_con_prints(self):
    print(f"Estado inicial: {self.objeto.estado}")
    resultado = self.objeto.operacion()
    print(f"Resultado: {resultado}")
    print(f"Estado final: {self.objeto.estado}")
    
    self.assertEqual(resultado, valor_esperado)
```

### ConfiguraciÃ³n de Logging

```python
import logging

def setUp(self):
    logging.basicConfig(level=logging.DEBUG)
    self.logger = logging.getLogger(__name__)

def test_con_logging(self):
    self.logger.debug("Iniciando prueba")
    resultado = operacion_bajo_prueba()
    self.logger.info(f"Resultado obtenido: {resultado}")
```

## AutomatizaciÃ³n

### IntegraciÃ³n Continua

Para integrar con sistemas de CI/CD:

```yaml
# Ejemplo para GitHub Actions
- name: Ejecutar Pruebas
  run: |
    python tests/run_tests.py
    
- name: Verificar Cobertura
  run: |
    python -m coverage run tests/run_tests.py
    python -m coverage report --fail-under=90
```

### Scripts de Pre-commit

```bash
#!/bin/bash
# pre-commit-hook.sh
echo "Ejecutando pruebas antes del commit..."
python tests/run_tests.py --unit

if [ $? -eq 0 ]; then
    echo "âœ… Todas las pruebas pasaron"
    exit 0
else
    echo "âŒ Pruebas fallaron - commit bloqueado"
    exit 1
fi
```

Las pruebas estÃ¡n diseÃ±adas para ser completas, rÃ¡pidas y confiables. Â¡MantÃ©n la cobertura alta y agrega pruebas para nuevas funcionalidades!
